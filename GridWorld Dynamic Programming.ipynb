{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "class for defining the GridWorld (4x4) Environment and its variables"
      ],
      "metadata": {
        "id": "TooW6DINukZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ypQAW_wQtyql"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, size=4):\n",
        "        self.size = size\n",
        "        self.states = [(i, j) for i in range(size) for j in range(size)]\n",
        "        self.actions = ['U', 'D', 'L', 'R']\n",
        "        self.terminal_states = [(0, 0), (size - 1, size - 1)]\n",
        "        self.gamma = 1.0\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        return state in self.terminal_states\n",
        "\n",
        "    def next_state(self, state, action):\n",
        "        if self.is_terminal(state):\n",
        "            return state, 0\n",
        "        i, j = state\n",
        "        if action == 'U':\n",
        "            i = max(i - 1, 0)\n",
        "        elif action == 'D':\n",
        "            i = min(i + 1, self.size - 1)\n",
        "        elif action == 'L':\n",
        "            j = max(j - 1, 0)\n",
        "        elif action == 'R':\n",
        "            j = min(j + 1, self.size - 1)\n",
        "        new_state = (i, j)\n",
        "        reward = 0 if new_state in self.terminal_states else -1\n",
        "        return new_state, reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterative Policy Evaluation to calculate the number of iterations needed to converge the values"
      ],
      "metadata": {
        "id": "BCSBNKBguu-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_random_policy(env):\n",
        "    policy = {}\n",
        "    for state in env.states:\n",
        "        if env.is_terminal(state):\n",
        "            policy[state] = {}\n",
        "        else:\n",
        "            policy[state] = {a: 0.25 for a in env.actions}\n",
        "    return policy\n",
        "\n",
        "def policy_evaluation(env, policy, theta=1e-4):\n",
        "    V = np.zeros((env.size, env.size))\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in env.states:\n",
        "            if env.is_terminal(state):\n",
        "                continue\n",
        "            v = 0\n",
        "            for action, action_prob in policy[state].items():\n",
        "                next_state, reward = env.next_state(state, action)\n",
        "                v += action_prob * (reward + env.gamma * V[next_state])\n",
        "            delta = max(delta, np.abs(v - V[state]))\n",
        "            V[state] = v\n",
        "        iteration += 1\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V, iteration\n",
        "\n",
        "# Example Usage\n",
        "env = GridWorld()\n",
        "policy = initialize_random_policy(env)\n",
        "V, iterations = policy_evaluation(env, policy)\n",
        "print(\"Value Function after Policy Evaluation:\\n\", V)\n",
        "print(\"Iterations to Converge:\", iterations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUlo-oj5um4c",
        "outputId": "38059acf-68d4-4e52-c3ba-300f6fb2a2fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Function after Policy Evaluation:\n",
            " [[  0.         -12.99934883 -18.99906386 -20.9989696 ]\n",
            " [-12.99934883 -16.99920093 -18.99913239 -18.99914232]\n",
            " [-18.99906386 -18.99913239 -16.9992679  -12.9994534 ]\n",
            " [-20.9989696  -18.99914232 -12.9994534    0.        ]]\n",
            "Iterations to Converge: 114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Iteration using Policy Evaluation"
      ],
      "metadata": {
        "id": "6FZJP0RSuyOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env):\n",
        "    policy = initialize_random_policy(env)\n",
        "    while True:\n",
        "        V, _ = policy_evaluation(env, policy)\n",
        "        policy_stable = True\n",
        "        for state in env.states:\n",
        "            if env.is_terminal(state):\n",
        "                continue\n",
        "            old_action = max(policy[state], key=policy[state].get)\n",
        "            action_values = {}\n",
        "            for action in env.actions:\n",
        "                next_state, reward = env.next_state(state, action)\n",
        "                action_values[action] = reward + env.gamma * V[next_state]\n",
        "            best_action = max(action_values, key=action_values.get)\n",
        "            new_policy = {a: (1.0 if a == best_action else 0.0) for a in env.actions}\n",
        "            if policy[state] != new_policy:\n",
        "                policy_stable = False\n",
        "            policy[state] = new_policy\n",
        "        if policy_stable:\n",
        "            break\n",
        "    return policy, V\n",
        "\n",
        "# Example Usage\n",
        "optimal_policy, optimal_V = policy_iteration(env)\n",
        "print(\"Optimal Policy from Policy Iteration:\")\n",
        "for state in sorted(optimal_policy.keys()):\n",
        "    print(f\"{state}: {optimal_policy[state]}\")\n",
        "print(\"\\nOptimal Value Function:\\n\", optimal_V)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swA0k9f4upDB",
        "outputId": "d6a3d552-bee3-4f40-bf49-b3ac5e736bee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy from Policy Iteration:\n",
            "(0, 0): {}\n",
            "(0, 1): {'U': 0.0, 'D': 0.0, 'L': 1.0, 'R': 0.0}\n",
            "(0, 2): {'U': 0.0, 'D': 0.0, 'L': 1.0, 'R': 0.0}\n",
            "(0, 3): {'U': 0.0, 'D': 1.0, 'L': 0.0, 'R': 0.0}\n",
            "(1, 0): {'U': 1.0, 'D': 0.0, 'L': 0.0, 'R': 0.0}\n",
            "(1, 1): {'U': 1.0, 'D': 0.0, 'L': 0.0, 'R': 0.0}\n",
            "(1, 2): {'U': 1.0, 'D': 0.0, 'L': 0.0, 'R': 0.0}\n",
            "(1, 3): {'U': 0.0, 'D': 1.0, 'L': 0.0, 'R': 0.0}\n",
            "(2, 0): {'U': 1.0, 'D': 0.0, 'L': 0.0, 'R': 0.0}\n",
            "(2, 1): {'U': 1.0, 'D': 0.0, 'L': 0.0, 'R': 0.0}\n",
            "(2, 2): {'U': 0.0, 'D': 1.0, 'L': 0.0, 'R': 0.0}\n",
            "(2, 3): {'U': 0.0, 'D': 1.0, 'L': 0.0, 'R': 0.0}\n",
            "(3, 0): {'U': 1.0, 'D': 0.0, 'L': 0.0, 'R': 0.0}\n",
            "(3, 1): {'U': 0.0, 'D': 0.0, 'L': 0.0, 'R': 1.0}\n",
            "(3, 2): {'U': 0.0, 'D': 0.0, 'L': 0.0, 'R': 1.0}\n",
            "(3, 3): {}\n",
            "\n",
            "Optimal Value Function:\n",
            " [[ 0.  0. -1. -2.]\n",
            " [ 0. -1. -2. -1.]\n",
            " [-1. -2. -1.  0.]\n",
            " [-2. -1.  0.  0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value Iteration"
      ],
      "metadata": {
        "id": "aeR6FanUu134"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, theta=1e-4):\n",
        "    V = np.zeros((env.size, env.size))\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in env.states:\n",
        "            if env.is_terminal(state):\n",
        "                continue\n",
        "            action_values = []\n",
        "            for action in env.actions:\n",
        "                next_state, reward = env.next_state(state, action)\n",
        "                action_values.append(reward + env.gamma * V[next_state])\n",
        "            max_value = max(action_values)\n",
        "            delta = max(delta, np.abs(max_value - V[state]))\n",
        "            V[state] = max_value\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    policy = {}\n",
        "    for state in env.states:\n",
        "        if env.is_terminal(state):\n",
        "            policy[state] = {}\n",
        "            continue\n",
        "        action_values = {}\n",
        "        for action in env.actions:\n",
        "            next_state, reward = env.next_state(state, action)\n",
        "            action_values[action] = reward + env.gamma * V[next_state]\n",
        "        best_action = max(action_values, key=action_values.get)\n",
        "        policy[state] = {a: (1.0 if a == best_action else 0.0) for a in env.actions}\n",
        "    return policy, V\n",
        "\n",
        "# Example Usage\n",
        "value_policy, value_V = value_iteration(env)\n",
        "print(\"Optimal Policy from Value Iteration:\")\n",
        "for state in sorted(value_policy.keys()):\n",
        "    print(f\"{state}: {value_policy[state]}\")\n",
        "print(\"\\nOptimal Value Function:\\n\", value_V)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GbrtLhnuzhd",
        "outputId": "6f6c112c-f115-467d-a4ae-f1757d93e9eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy from Value Iteration:\n",
            "(0, 0): {}\n",
            "(0, 1): {'U': 0.0, 'D': 0.0, 'L': 1.0, 'R': 0.0}\n",
            "(0, 2): {'U': 0.0, 'D': 0.0, 'L': 1.0, 'R': 0.0}\n",
            "(0, 3): {'U': 0.0, 'D': 1.0, 'L': 0.0, 'R': 0.0}\n",
            "(1, 0): {'U': 1.0, 'D': 0.0, 'L': 0.0, 'R': 0.0}\n",
            "(1, 1): {'U': 1.0, 'D': 0.0, 'L': 0.0, 'R': 0.0}\n",
            "(1, 2): {'U': 1.0, 'D': 0.0, 'L': 0.0, 'R': 0.0}\n",
            "(1, 3): {'U': 0.0, 'D': 1.0, 'L': 0.0, 'R': 0.0}\n",
            "(2, 0): {'U': 1.0, 'D': 0.0, 'L': 0.0, 'R': 0.0}\n",
            "(2, 1): {'U': 1.0, 'D': 0.0, 'L': 0.0, 'R': 0.0}\n",
            "(2, 2): {'U': 0.0, 'D': 1.0, 'L': 0.0, 'R': 0.0}\n",
            "(2, 3): {'U': 0.0, 'D': 1.0, 'L': 0.0, 'R': 0.0}\n",
            "(3, 0): {'U': 1.0, 'D': 0.0, 'L': 0.0, 'R': 0.0}\n",
            "(3, 1): {'U': 0.0, 'D': 0.0, 'L': 0.0, 'R': 1.0}\n",
            "(3, 2): {'U': 0.0, 'D': 0.0, 'L': 0.0, 'R': 1.0}\n",
            "(3, 3): {}\n",
            "\n",
            "Optimal Value Function:\n",
            " [[ 0.  0. -1. -2.]\n",
            " [ 0. -1. -2. -1.]\n",
            " [-1. -2. -1.  0.]\n",
            " [-2. -1.  0.  0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "In this experiment, we successfully implemented and analyzed two dynamic programming techniques — Policy Iteration and Value Iteration — to solve the classic GridWorld problem in Reinforcement Learning. We began by defining the environment as a 4x4 grid, modeling the transitions, rewards, and terminal states. Using an initial random policy, we evaluated the value function iteratively until convergence and recorded the number of iterations required. This helped us understand the process of policy evaluation and how value functions evolve with repeated updates. Further, we implemented Policy Iteration by alternating between policy evaluation and policy improvement steps, ultimately leading to an optimal policy that maximizes cumulative rewards. We then applied Value Iteration, which combines evaluation and improvement in a single update step, and observed that it yielded similar optimal values and policies more efficiently. Through this lab, we gained hands-on experience with Markov Decision Processes (MDPs), the Bellman equations, and the foundational ideas of dynamic programming in RL, which are crucial for building more advanced agents in real-world environments."
      ],
      "metadata": {
        "id": "QxIXE-lGu_qC"
      }
    }
  ]
}